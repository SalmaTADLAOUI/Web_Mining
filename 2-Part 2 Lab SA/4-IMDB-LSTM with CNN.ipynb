{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use \"IMDB movie review sentiment classification dataset\"\n",
    "\n",
    "Dataset Description: https://keras.io/api/datasets/imdb/\n",
    "\n",
    "This is a dataset of 25,000 movie reviews from IMDB, tagged by sentiment (positive/negative). The reviews have been preprocessed and each review is coded as a list of (whole) word indexes. For convenience, words are indexed by their overall frequency in the dataset, so that, for example, the integer \"3\" encodes the 3rd most frequent word in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import keras\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Embedding, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use the embedding layer which defines the first hidden layer of the network. it must specify 3 arguments:\n",
    "\n",
    "input_dim: the size of the vocabulary in the text\n",
    "\n",
    "output_dim: this is the size of the vector space in which each word will be immersed\n",
    "\n",
    "input_legth: this is the size of the sequence, for example if your documents contain 100 words each then it is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/6\n",
      "391/391 [==============================] - 186s 469ms/step - loss: 0.4308 - accuracy: 0.7946 - val_loss: 0.3179 - val_accuracy: 0.8673\n",
      "Epoch 2/6\n",
      "391/391 [==============================] - 203s 520ms/step - loss: 0.2505 - accuracy: 0.9010 - val_loss: 0.2834 - val_accuracy: 0.8817\n",
      "Epoch 3/6\n",
      "391/391 [==============================] - 199s 510ms/step - loss: 0.2099 - accuracy: 0.9198 - val_loss: 0.2896 - val_accuracy: 0.8815\n",
      "Epoch 4/6\n",
      "391/391 [==============================] - 209s 535ms/step - loss: 0.1722 - accuracy: 0.9363 - val_loss: 0.3155 - val_accuracy: 0.8836\n",
      "Epoch 5/6\n",
      "391/391 [==============================] - 200s 512ms/step - loss: 0.1470 - accuracy: 0.9473 - val_loss: 0.3419 - val_accuracy: 0.8756\n",
      "Epoch 6/6\n",
      "391/391 [==============================] - 162s 413ms/step - loss: 0.1183 - accuracy: 0.9604 - val_loss: 0.3372 - val_accuracy: 0.8804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f6aff77f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating tyhe model \n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=6, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.04%\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a simple example of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1,1,1,1,1,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40, 45], [31, 11], [9, 39], [5, 11], [47], [8], [2, 39], [14, 31], [2, 11], [9, 19, 45, 39]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[40 45  0  0]\n",
      " [31 11  0  0]\n",
      " [ 9 39  0  0]\n",
      " [ 5 11  0  0]\n",
      " [47  0  0  0]\n",
      " [ 8  0  0  0]\n",
      " [ 2 39  0  0]\n",
      " [14 31  0  0]\n",
      " [ 2 11  0  0]\n",
      " [ 9 19 45 39]]\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to define our Embedding layer as part of our model.\n",
    "\n",
    "The embedding has a vocabulary of 50 and an entry length of 4. We will choose a small embedding space of 8 dimensions.\n",
    "\n",
    "The model is a simple binary classification model. It is important to note that the output of the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten it (the flatten layer) into a 32-element vector to pass it to the Dense output layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 4, 8)              400       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23f6f81c190>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = np.array(labels)\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.000001\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do: \n",
    "\n",
    "1. Try the same thing on Google reviews dataset ( the file is given in the lab directory)\n",
    "2. try to change the embedding representation using Glove and Skipgram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15746, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>sortOrder</th>\n",
       "      <th>appId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andrew Thomas</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
       "      <td>Update: After getting a response from the deve...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-05 22:25:57</td>\n",
       "      <td>According to our TOS, and the term you have ag...</td>\n",
       "      <td>2020-04-05 15:10:24</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Craig Haines</td>\n",
       "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
       "      <td>Used it for a fair amount of time without any ...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-04 13:40:01</td>\n",
       "      <td>It sounds like you logged in with a different ...</td>\n",
       "      <td>2020-04-05 15:11:35</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steven adkins</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
       "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-01 16:18:13</td>\n",
       "      <td>This sounds odd! We are not aware of any issue...</td>\n",
       "      <td>2020-04-02 16:05:56</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lars Panzerbjørn</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
       "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>4.17.0.2</td>\n",
       "      <td>2020-03-12 08:17:34</td>\n",
       "      <td>We do offer this option as part of the Advance...</td>\n",
       "      <td>2020-03-15 06:20:13</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scott Prewitt</td>\n",
       "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
       "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>4.17.0.2</td>\n",
       "      <td>2020-03-14 17:41:01</td>\n",
       "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
       "      <td>2020-03-15 23:45:51</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userName                                          userImage  \\\n",
       "0     Andrew Thomas  https://lh3.googleusercontent.com/a-/AOh14GiHd...   \n",
       "1      Craig Haines  https://lh3.googleusercontent.com/-hoe0kwSJgPQ...   \n",
       "2     steven adkins  https://lh3.googleusercontent.com/a-/AOh14GiXw...   \n",
       "3  Lars Panzerbjørn  https://lh3.googleusercontent.com/a-/AOh14Gg-h...   \n",
       "4     Scott Prewitt  https://lh3.googleusercontent.com/-K-X1-YsVd6U...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0  Update: After getting a response from the deve...      1             21   \n",
       "1  Used it for a fair amount of time without any ...      1             11   \n",
       "2  Your app sucks now!!!!! Used to be good but no...      1             17   \n",
       "3  It seems OK, but very basic. Recurring tasks n...      1            192   \n",
       "4  Absolutely worthless. This app runs a prohibit...      1             42   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0             4.17.0.3  2020-04-05 22:25:57   \n",
       "1             4.17.0.3  2020-04-04 13:40:01   \n",
       "2             4.17.0.3  2020-04-01 16:18:13   \n",
       "3             4.17.0.2  2020-03-12 08:17:34   \n",
       "4             4.17.0.2  2020-03-14 17:41:01   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0  According to our TOS, and the term you have ag...  2020-04-05 15:10:24   \n",
       "1  It sounds like you logged in with a different ...  2020-04-05 15:11:35   \n",
       "2  This sounds odd! We are not aware of any issue...  2020-04-02 16:05:56   \n",
       "3  We do offer this option as part of the Advance...  2020-03-15 06:20:13   \n",
       "4  We're sorry you feel this way! 90% of the app ...  2020-03-15 23:45:51   \n",
       "\n",
       "       sortOrder      appId  \n",
       "0  most_relevant  com.anydo  \n",
       "1  most_relevant  com.anydo  \n",
       "2  most_relevant  com.anydo  \n",
       "3  most_relevant  com.anydo  \n",
       "4  most_relevant  com.anydo  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import spacy\n",
    "import gensim\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Update: After getting a response from the deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Used it for a fair amount of time without any ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15741</th>\n",
       "      <td>I believe that this is by far the best app wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15742</th>\n",
       "      <td>It sometimes crashes a lot!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15743</th>\n",
       "      <td>Works well for what I need</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15744</th>\n",
       "      <td>Love it.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15745</th>\n",
       "      <td>Really amazing and helped me sooo much just i ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15746 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  sentiment\n",
       "0      Update: After getting a response from the deve...          0\n",
       "1      Used it for a fair amount of time without any ...          0\n",
       "2      Your app sucks now!!!!! Used to be good but no...          0\n",
       "3      It seems OK, but very basic. Recurring tasks n...          0\n",
       "4      Absolutely worthless. This app runs a prohibit...          0\n",
       "...                                                  ...        ...\n",
       "15741  I believe that this is by far the best app wit...          1\n",
       "15742                       It sometimes crashes a lot!!          1\n",
       "15743                         Works well for what I need          1\n",
       "15744                                           Love it.          1\n",
       "15745  Really amazing and helped me sooo much just i ...          1\n",
       "\n",
       "[15746 rows x 2 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_sentiment(score):\n",
    "    score = int(score)\n",
    "    if score <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "df['sentiment'] = df.score.apply(to_sentiment)\n",
    "\n",
    "df = df[['content', 'sentiment']]\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_23 (Embedding)    (None, 391, 50)           598500    \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 391, 32)           4832      \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPoolin  (None, 195, 32)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " lstm_19 (LSTM)              (None, 128)               82432     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 685,893\n",
      "Trainable params: 87,393\n",
      "Non-trainable params: 598,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "394/394 [==============================] - 120s 288ms/step - loss: 0.5667 - accuracy: 0.7078 - val_loss: 0.5110 - val_accuracy: 0.7467\n",
      "Epoch 2/20\n",
      "394/394 [==============================] - 110s 279ms/step - loss: 0.4875 - accuracy: 0.7547 - val_loss: 0.4557 - val_accuracy: 0.7775\n",
      "Epoch 3/20\n",
      "394/394 [==============================] - 120s 304ms/step - loss: 0.4580 - accuracy: 0.7787 - val_loss: 0.4553 - val_accuracy: 0.7863\n",
      "Epoch 4/20\n",
      "394/394 [==============================] - 121s 308ms/step - loss: 0.4400 - accuracy: 0.7898 - val_loss: 0.4666 - val_accuracy: 0.7841\n",
      "Epoch 5/20\n",
      "394/394 [==============================] - 121s 308ms/step - loss: 0.4119 - accuracy: 0.8076 - val_loss: 0.4296 - val_accuracy: 0.7984\n",
      "Epoch 6/20\n",
      "394/394 [==============================] - 121s 308ms/step - loss: 0.3908 - accuracy: 0.8193 - val_loss: 0.4257 - val_accuracy: 0.8111\n",
      "Epoch 7/20\n",
      "394/394 [==============================] - 127s 322ms/step - loss: 0.3643 - accuracy: 0.8327 - val_loss: 0.4364 - val_accuracy: 0.8006\n",
      "Epoch 8/20\n",
      "394/394 [==============================] - 121s 308ms/step - loss: 0.3341 - accuracy: 0.8550 - val_loss: 0.4100 - val_accuracy: 0.8216\n",
      "Epoch 9/20\n",
      "394/394 [==============================] - 120s 306ms/step - loss: 0.3076 - accuracy: 0.8684 - val_loss: 0.4301 - val_accuracy: 0.8063\n",
      "Epoch 10/20\n",
      "394/394 [==============================] - 126s 319ms/step - loss: 0.2757 - accuracy: 0.8847 - val_loss: 0.4148 - val_accuracy: 0.8289\n",
      "Epoch 11/20\n",
      "394/394 [==============================] - 122s 309ms/step - loss: 0.2434 - accuracy: 0.9013 - val_loss: 0.4540 - val_accuracy: 0.8175\n",
      "Epoch 12/20\n",
      "394/394 [==============================] - 115s 293ms/step - loss: 0.2254 - accuracy: 0.9096 - val_loss: 0.4301 - val_accuracy: 0.8483\n",
      "Epoch 13/20\n",
      "394/394 [==============================] - 110s 278ms/step - loss: 0.1857 - accuracy: 0.9302 - val_loss: 0.4272 - val_accuracy: 0.8619\n",
      "Epoch 14/20\n",
      "394/394 [==============================] - 107s 271ms/step - loss: 0.1531 - accuracy: 0.9430 - val_loss: 0.4524 - val_accuracy: 0.8641\n",
      "Epoch 15/20\n",
      "394/394 [==============================] - 115s 293ms/step - loss: 0.1349 - accuracy: 0.9512 - val_loss: 0.4642 - val_accuracy: 0.8590\n",
      "Epoch 16/20\n",
      "394/394 [==============================] - 127s 323ms/step - loss: 0.1145 - accuracy: 0.9590 - val_loss: 0.4620 - val_accuracy: 0.8708\n",
      "Epoch 17/20\n",
      "394/394 [==============================] - 126s 321ms/step - loss: 0.0959 - accuracy: 0.9648 - val_loss: 0.4901 - val_accuracy: 0.8711\n",
      "Epoch 18/20\n",
      "394/394 [==============================] - 122s 311ms/step - loss: 0.0850 - accuracy: 0.9694 - val_loss: 0.5218 - val_accuracy: 0.8737\n",
      "Epoch 19/20\n",
      "394/394 [==============================] - 113s 287ms/step - loss: 0.0693 - accuracy: 0.9765 - val_loss: 0.5786 - val_accuracy: 0.8765\n",
      "Epoch 20/20\n",
      "394/394 [==============================] - 105s 266ms/step - loss: 0.0669 - accuracy: 0.9762 - val_loss: 0.6130 - val_accuracy: 0.8708\n",
      "Test Loss: 0.6130127906799316\n",
      "Test Accuracy: 0.8707936406135559\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Calculate the maximum sequence length\n",
    "max_sequence_length = max(len(content.split()) for content in df[\"content\"])\n",
    "\n",
    "# Tokenize the content\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(df[\"content\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"content\"])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "content_embeddings = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(content_embeddings, df[\"sentiment\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_embeddings = {}\n",
    "with open('C:\\glove.6B.50d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < num_words:\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Define the LSTM model\n",
    "\n",
    "\n",
    "# creating tyhe model \n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding with Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "394/394 [==============================] - 136s 340ms/step - loss: 0.5450 - accuracy: 0.7371 - val_loss: 0.5011 - val_accuracy: 0.7663\n",
      "Epoch 2/10\n",
      "394/394 [==============================] - 129s 329ms/step - loss: 0.4800 - accuracy: 0.7776 - val_loss: 0.4908 - val_accuracy: 0.7705\n",
      "Epoch 3/10\n",
      "394/394 [==============================] - 133s 337ms/step - loss: 0.4564 - accuracy: 0.7880 - val_loss: 0.4642 - val_accuracy: 0.7829\n",
      "Epoch 4/10\n",
      "394/394 [==============================] - 141s 358ms/step - loss: 0.4569 - accuracy: 0.7932 - val_loss: 0.5499 - val_accuracy: 0.7260\n",
      "Epoch 5/10\n",
      "394/394 [==============================] - 138s 351ms/step - loss: 0.4213 - accuracy: 0.8094 - val_loss: 0.4375 - val_accuracy: 0.8013\n",
      "Epoch 6/10\n",
      "394/394 [==============================] - 152s 386ms/step - loss: 0.4004 - accuracy: 0.8216 - val_loss: 0.4224 - val_accuracy: 0.8121\n",
      "Epoch 7/10\n",
      "394/394 [==============================] - 134s 340ms/step - loss: 0.3726 - accuracy: 0.8365 - val_loss: 0.4095 - val_accuracy: 0.8210\n",
      "Epoch 8/10\n",
      "394/394 [==============================] - 134s 340ms/step - loss: 0.3532 - accuracy: 0.8454 - val_loss: 0.4195 - val_accuracy: 0.8149\n",
      "Epoch 9/10\n",
      "394/394 [==============================] - 134s 340ms/step - loss: 0.3255 - accuracy: 0.8606 - val_loss: 0.3988 - val_accuracy: 0.8222\n",
      "Epoch 10/10\n",
      "394/394 [==============================] - 137s 348ms/step - loss: 0.2973 - accuracy: 0.8735 - val_loss: 0.3885 - val_accuracy: 0.8343\n",
      "Test Loss: 0.3885018229484558\n",
      "Test Accuracy: 0.8342857360839844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Calculate the maximum sequence length\n",
    "max_sequence_length = max(len(content.split()) for content in df[\"content\"])\n",
    "\n",
    "# Tokenize the content\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(df[\"content\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"content\"])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "content_embeddings = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(content_embeddings, df[\"sentiment\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "glove_embeddings = {}\n",
    "with open('C:\\glove.6B.50d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < num_words:\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding with Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "394/394 [==============================] - 188s 469ms/step - loss: 0.6518 - accuracy: 0.6423 - val_loss: 0.6733 - val_accuracy: 0.6276\n",
      "Epoch 2/10\n",
      "394/394 [==============================] - 150s 381ms/step - loss: 0.6489 - accuracy: 0.6442 - val_loss: 0.6571 - val_accuracy: 0.6273\n",
      "Epoch 3/10\n",
      "394/394 [==============================] - 148s 376ms/step - loss: 0.6477 - accuracy: 0.6446 - val_loss: 0.6572 - val_accuracy: 0.6286\n",
      "Epoch 4/10\n",
      "394/394 [==============================] - 151s 383ms/step - loss: 0.6472 - accuracy: 0.6446 - val_loss: 0.6566 - val_accuracy: 0.6289\n",
      "Epoch 5/10\n",
      "394/394 [==============================] - 139s 353ms/step - loss: 0.6458 - accuracy: 0.6450 - val_loss: 0.6564 - val_accuracy: 0.6292\n",
      "Epoch 6/10\n",
      "394/394 [==============================] - 159s 404ms/step - loss: 0.6453 - accuracy: 0.6453 - val_loss: 0.6591 - val_accuracy: 0.6292\n",
      "Epoch 7/10\n",
      "394/394 [==============================] - 150s 381ms/step - loss: 0.6438 - accuracy: 0.6457 - val_loss: 0.6560 - val_accuracy: 0.6292\n",
      "Epoch 8/10\n",
      "394/394 [==============================] - 166s 422ms/step - loss: 0.6447 - accuracy: 0.6440 - val_loss: 0.6557 - val_accuracy: 0.6292\n",
      "Epoch 9/10\n",
      "394/394 [==============================] - 160s 406ms/step - loss: 0.6430 - accuracy: 0.6459 - val_loss: 0.6557 - val_accuracy: 0.6295\n",
      "Epoch 10/10\n",
      "394/394 [==============================] - 170s 431ms/step - loss: 0.6417 - accuracy: 0.6470 - val_loss: 0.6586 - val_accuracy: 0.6292\n",
      "Test Loss: 0.6585848927497864\n",
      "Test Accuracy: 0.6292063593864441\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Calculate the maximum sequence length\n",
    "max_sequence_length = max(len(content.split()) for content in df[\"content\"])\n",
    "\n",
    "# Tokenize the content\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(df[\"content\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"content\"])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "content_embeddings = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(content_embeddings, df[\"sentiment\"], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Word2Vec model on the content\n",
    "word2vec_model = Word2Vec(sentences=df[\"content\"], vector_size=embedding_dim, window=5, min_count=1, sg=1)\n",
    "word2vec_model.train(df[\"content\"], total_examples=len(df[\"content\"]), epochs=10)\n",
    "\n",
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index < num_words:\n",
    "        if word in word2vec_model.wv:\n",
    "            embedding_matrix[index] = word2vec_model.wv[word]\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Glove Embedding word model gives better results than SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
